# Backend Configuration
HUGGING_FACE_TOKEN=your_token_here
USE_LOCAL_LLM=false

# API Keys for LLM Services
MISTRAL_API_KEY=your_mistral_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here

# LLM Provider Configuration
# Options: "gemini", "mistral", "ollama", "auto"
LLM_PROVIDER=gemini
# Model name (optional, uses defaults if not specified)
LLM_MODEL_NAME=gemini

# Frontend Configuration  
REACT_APP_API_URL=http://localhost:8000/api/v1

# Optional: Set to true if you want to use local Ollama
# Make sure to run: ollama serve && ollama run llama3
# USE_LOCAL_LLM=true
